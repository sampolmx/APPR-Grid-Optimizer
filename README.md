# APPR-Grid-Optimizer
Motor de optimizaciÃ³n basado en Reinforcement Learning (DQN) para la gestiÃ³n dinÃ¡mica de la capacidad de transmisiÃ³n elÃ©ctrica. Reduce el desperdicio de energÃ­a solar (*curtailment*) al gestionar proactivamente el almacenamiento de baterÃ­as
# APPR-Grid-Optimizer

![Badge de estado de desarrollo](https://img.shields.io/badge/Fase%20Actual-Entrenamiento%20DQN-blue)
![Badge de lenguaje](https://img.shields.io/badge/Lenguaje-Python-yellow.svg)
![Badge de framework](https://img.shields.io/badge/Framework-TensorFlow%20%2F%20Keras-orange.svg)

## ğŸŒŸ VisiÃ³n General

Este repositorio alberga el prototipo del **Agente de PlanificaciÃ³n Predictiva de Red (APPR)**. El APPR es un sistema basado en **Aprendizaje por Refuerzo (Deep Q-Network - DQN)** diseÃ±ado para resolver uno de los cuellos de botella mÃ¡s crÃ­ticos en la transiciÃ³n energÃ©tica: la gestiÃ³n de la intermitencia renovable.

El agente aprende a despachar dinÃ¡micamente recursos de almacenamiento (baterÃ­as) para **minimizar el *curtailment*** (desperdicio de energÃ­a solar) mientras se adhiere estrictamente a un **lÃ­mite de capacidad de transmisiÃ³n fijo** (simulando un cuello de botella).

### ğŸ¯ Objetivo EstratÃ©gico

Convertirse en un **Optimizador de la TransiciÃ³n**, reduciendo la fricciÃ³n tÃ©cnica y econÃ³mica que ralentiza la adopciÃ³n masiva de energÃ­as limpias.

## ğŸš€ Estado del Proyecto (MVP)

El prototipo MVP se centra en una simulaciÃ³n controlada:

*   **Sistema:** 100 MW de capacidad solar instalada.
*   **Cuello de Botella:** LÃ­mite de transmisiÃ³n estricto de **80 MW**.
*   **Recurso de MitigaciÃ³n:** BaterÃ­a de 60 MWh con tasa de 20 MW.
*   **Fase:** Entrenamiento del agente DQN completado, comparando la polÃ­tica aprendida contra una gestiÃ³n ingenua (*Baseline*).

## ğŸ› ï¸ CÃ³mo Ejecutar el Prototipo

Este proyecto estÃ¡ diseÃ±ado para ejecutarse en un entorno Jupyter Notebook.

### 1. Prerrequisitos

AsegÃºrese de tener Python y Jupyter instalados.

### 2. InstalaciÃ³n de Dependencias

Instale todas las librerÃ­as necesarias a partir del archivo `requirements.txt`:

```bash
pip install -r requirements.txt
```

### 3. EjecuciÃ³n de los Notebooks

Ejecute los siguientes notebooks en orden secuencial dentro de la carpeta `/notebooks`:

1.  **`01_Data_Prep_Baseline.ipynb`**: Define el entorno simulado, genera el conjunto de datos de estrÃ©s y establece la mÃ©trica de comparaciÃ³n (*Baseline*).
2.  **`02_APPR_DQN_Training.ipynb`**: Contiene la implementaciÃ³n del entorno RL, el modelo DQN (TensorFlow/Keras) y el bucle de entrenamiento.

---

## ğŸ“ Estructura del Repositorio

```
/APPR_Grid_Optimizer
â”œâ”€â”€ .gitignore             # Archivos ignorados (cachÃ©s, datos brutos grandes)
â”œâ”€â”€ requirements.txt       # Lista de dependencias para replicaciÃ³n
â”œâ”€â”€ README.md              # DocumentaciÃ³n actual
â”‚
â”œâ”€â”€ data/                  # (Se puede usar para datos reales si son necesarios)
â”‚
â””â”€â”€ notebooks/
    â”œâ”€â”€ 01_Data_Prep_Baseline.ipynb
    â””â”€â”€ 02_APPR_DQN_Training.ipynb
```

## âš™ï¸ PrÃ³ximos Pasos (Hoja de Ruta)

1.  **RefactorizaciÃ³n y ValidaciÃ³n:** Mejorar la Fase 3 para obtener una comparaciÃ³n visual y cuantitativa directa entre Baseline y APPR.
2.  **IntegraciÃ³n de PredicciÃ³n:** Migrar el estado del agente para incluir modelos de pronÃ³stico de energÃ­a (usando LSTMs o Transformers) en lugar de solo datos instantÃ¡neos.
3.  **Escalabilidad a GCP:** Migrar la lÃ³gica del entorno y el entrenamiento a un servicio gestionado (ej. Vertex AI Training) para simular escenarios mÃ¡s grandes y complejos.

---
*Desarrollado con el objetivo de acelerar la adopciÃ³n de energÃ­a limpia mediante optimizaciÃ³n inteligente de sistemas.*
