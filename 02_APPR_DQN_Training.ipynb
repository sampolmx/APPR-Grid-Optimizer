{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59d64c1e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append(os.path.abspath('..'))\n",
        "from src.appr_core import GridEnvironment, enrich_data_with_forecast\n",
        "from src.appr_agent import DQNAgent # PyTorch Agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b1f8103",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Re-importar dependencias de TensorFlow/Keras\n",
        "\n",
        "# --- RECARGA DE PARÁMETROS DEL ENTORNO (DEBE COINCIDIR CON EL NOTEBOOK ANTERIOR) ---\n",
        "# Si el notebook se ejecuta independientemente, necesitamos redefinir el DataFrame y parámetros:\n",
        "try:\n",
        "    # Intenta usar el df creado en el notebook anterior\n",
        "    df \n",
        "except NameError:\n",
        "    print(\"DataFrame 'df' no encontrado. Re-generando datos del caso de estrés...\")\n",
        "    # Si falla, generamos los datos de nuevo (simplificado para la prueba)\n",
        "    HORIZONTE_DIAS = 3\n",
        "    CAPACIDAD_INSTALADA_SOLAR = 100.0\n",
        "    CAPACIDAD_RESTRINGIDA = 80.0\n",
        "    HORAS = 24 * HORIZONTE_DIAS\n",
        "    index = pd.date_range(start='2024-12-21', periods=HORAS, freq='H')\n",
        "    df = pd.DataFrame(index=index)\n",
        "    # Simulación básica para que el código corra\n",
        "    df['Demanda_MW'\n",
        "] = 70 + 10 * np.sin(2 * np.pi * (df.index.hour) / 24)\n",
        "    df['Generacion_Solar_MW'\n",
        "] = 90 + 30 * np.sin(2 * np.pi * (df.index.hour - 6) / 48) \n",
        "    df['Exceso_Solar'\n",
        "] = df['Generacion_Solar_MW'\n",
        "] - CAPACIDAD_RESTRINGIDA\n",
        "    df['Curtailment_Baseline_MW'\n",
        "] = df['Exceso_Solar'\n",
        "].apply(lambda x: max(0, x))\n",
        "\n",
        "# --- PARÁMETROS DE LA BATERÍA ---\n",
        "CAPACIDAD_BATERIA_MWh = 60.0\n",
        "TASA_MAX_MW = 20.0 # Tasa máxima de carga/descarga permitida por hora\n",
        "SOC_INICIAL = CAPACIDAD_BATERIA_MWh / 2  # Empezamos a medio cargar\n",
        "\n",
        "# ====================================================================\n",
        "# PARTE 1: DEFINICIÓN DEL ENTORNO RL (GridEnvironment)\n",
        "# ====================================================================\n",
        "\n",
        "# GridEnvironment imported from src.appr_core\n",
        "# --- ENRICH DATA ---\n",
        "df = enrich_data_with_forecast(df)\n",
        "\n",
        "env = GridEnvironment(\n",
        "    df=df, \n",
        "    capacity_limit=CAPACIDAD_RESTRINGIDA, \n",
        "    battery_capacity=CAPACIDAD_BATERIA_MWh,\n",
        "    battery_rate=TASA_MAX_MW,\n",
        "    training_mode=True\n",
        ")\n",
        "\n",
        "print(f\"Entorno RL inicializado. Estado inicial (SOC): {env.soc:.2f} MWh.\")\n",
        "\n",
        "\n",
        "# ====================================================================\n",
        "# PARTE 2: IMPLEMENTACIÓN DEL AGENTE DQN (TensorFlow/Keras)\n",
        "# ====================================================================\n",
        "\n",
        "# --- Definición de Modelos y Hiperparámetros ---\n",
        "state_size = env.state_space_size\n",
        "action_size = env.action_space_size\n",
        "\n",
        "# Hiperparámetros\n",
        "EPISODES = 150 # Reducido para una prueba rápida de concepto\n",
        "GAMMA = 0.95\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_END = 0.05\n",
        "EPSILON_DECAY = (EPSILON_START - EPSILON_END) / EPISODES\n",
        "\n",
        "# Memoria de Experiencia\n",
        "memory = deque(maxlen=20000)\n",
        "\n",
        "# Función para construir la Q-Network\n",
        "def build_dqn(input_shape, output_shape):\n",
        "    model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(input_shape,)),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(output_shape, activation='linear')\n",
        "])\n",
        "    model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.005))\n",
        "    return model\n",
        "\n",
        "# Inicialización de Redes\n",
        "q_network = build_dqn(state_size, action_size)\n",
        "target_network = build_dqn(state_size, action_size)\n",
        "target_network.set_weights(q_network.get_weights()) # Sincronizar pesos iniciales\n",
        "\n",
        "# --- Bucle de Entrenamiento ---\n",
        "history = []\n",
        "epsilon = EPSILON_START\n",
        "print(f\"\\nIniciando entrenamiento del APPR para {EPISODES} episodios...\")\n",
        "\n",
        "for e in range(EPISODES):\n",
        "    state = env.reset()\n",
        "    episode_reward = 0\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        # 1. Elección de Acción (Exploración vs. Explotación)\n",
        "        if np.random.rand() <= epsilon:\n",
        "            action = random.randrange(env.action_space_size) # Exploración\n",
        "        else:\n",
        "            # Optimización: Usar __call__ en lugar de predict para inferencia rápida\n",
        "    # Usar agente para predicción\n",
        "    action = agent.act(state, training=False)\n",
        "    0\n",
        "]) # Explotación\n",
        "\n",
        "        # 2. Ejecutar la acción\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        \n",
        "        # 3. Almacenar experiencia\n",
        "        memory.append((state, action, reward, next_state, done))\n",
        "        \n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "        \n",
        "        # 4. Entrenamiento (Replay Batch)\n",
        "        if len(memory) > 100:\n",
        "            minibatch = random.sample(memory, min(64, len(memory)))\n",
        "            \n",
        "            states, actions, rewards, next_states, dones = zip(*minibatch)\n",
        "            states = np.array(states)\n",
        "            next_states = np.array(next_states)\n",
        "            \n",
        "            # Cálculo del Target Q-Value\n",
        "            target_q_next = target_network(next_states, training=False).numpy().max(axis=1)\n",
        "            target_q = rewards + GAMMA * target_q_next * (1 - np.array(dones))\n",
        "            \n",
        "            # Actualización de la Q-Network\n",
        "            target_f = q_network(states, training=False).numpy()\n",
        "            for i in range(len(actions)):\n",
        "                target_f[i\n",
        "][actions[i\n",
        "    ]\n",
        "] = target_q[i\n",
        "]\n",
        "            \n",
        "            # Optimización: train_on_batch es mucho más rápido que fit para un solo lote\n",
        "            q_network.train_on_batch(states, target_f)\n",
        "        \n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # 5. Actualización de Parámetros\n",
        "    epsilon = max(EPSILON_END, epsilon - EPSILON_DECAY)\n",
        "    \n",
        "    if e % 25 == 0:\n",
        "        target_network.set_weights(q_network.get_weights())\n",
        "        \n",
        "    history.append(episode_reward)\n",
        "    \n",
        "    if e % 25 == 0 or e == EPISODES - 1:\n",
        "        print(f\"Episodio {e}/{EPISODES} | Recompensa Total: {episode_reward:.2f} | Epsilon: {epsilon:.3f}\")\n",
        "\n",
        "\n",
        "# ====================================================================\n",
        "# PARTE 3: EVALUACIÓN FINAL DEL APRENDIZAJE\n",
        "# ====================================================================\n",
        "\n",
        "# 1. Ejecutar el entorno con la política entrenada (Explotación total)\n",
        "env.training_mode = False # Habilitar logging para evaluación\n",
        "env.reset()\n",
        "state = env._get_state()\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    # Usar la política entrenada (Explotación pura)\n",
        "    # Usar agente para predicción\n",
        "    action = agent.act(state, training=False)\n",
        "    0\n",
        "])\n",
        "    \n",
        "    # Ejecutar la acción\n",
        "    state, _, done, _ = env.step(action)\n",
        "\n",
        "# 2. Calcular el Desperdicio total del APPR\n",
        "total_curtailment_appr = df['Curtailment_APPR'\n",
        "].sum()\n",
        "total_curtailment_baseline = df['Curtailment_Baseline_MW'\n",
        "].sum()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"           RESULTADOS FINALES DE LA OPTIMIZACIÓN          \")\n",
        "print(\"=\"*50)\n",
        "print(f\"1. BaseLine (Corte Inmediato): {total_curtailment_baseline:.2f} MWh Desperdiciados\")\n",
        "print(f\"2. APPR (DQN Optimizado):     {total_curtailment_appr:.2f} MWh Desperdiciados\")\n",
        "\n",
        "if total_curtailment_baseline > 0:\n",
        "    reduccion = ((total_curtailment_baseline - total_curtailment_appr) / total_curtailment_baseline) * 100\n",
        "    print(f\"\\n✅ Mitigación lograda: {reduccion:.2f}% de reducción del desperdicio.\")\n",
        "else:\n",
        "    print(\"\\nAdvertencia: El escenario de estrés no fue lo suficientemente severo para el baseline.\")\n",
        "\n",
        "\n",
        "# 3. Visualización de la Recompensa y el SOC\n",
        "fig, (ax1, ax2) = plt.subplots(2,\n",
        "1, figsize=(15,\n",
        "8), sharex=True)\n",
        "\n",
        "# Gráfico 1: Recompensa durante el entrenamiento\n",
        "ax1.plot(history, label='Recompensa Total por Episodio', color='blue')\n",
        "ax1.set_title('Progreso del Entrenamiento del APPR (Recompensa)')\n",
        "ax1.set_ylabel('Recompensa Acumulada (Mayor es Mejor)')\n",
        "ax1.grid(True, alpha=0.4)\n",
        "\n",
        "# Gráfico 2: Comparación de Inyección (Solo 24h críticas para claridad)\n",
        "df_plot = df.iloc[\n",
        "    24: 48\n",
        "].copy() # Tomamos el segundo día como ejemplo\n",
        "df_plot['Inyeccion_APPR'\n",
        "] = df_plot['Generacion_Solar_MW'\n",
        "] - df_plot['Curtailment_APPR'\n",
        "]\n",
        "df_plot['Inyeccion_Baseline_MW'\n",
        "] = df_plot['Generacion_Solar_MW'\n",
        "] - df_plot['Curtailment_Baseline_MW'\n",
        "]\n",
        "\n",
        "\n",
        "ax2.plot(df_plot.index, df_plot['Demanda_MW'\n",
        "], label='Demanda', color='gray', alpha=0.7)\n",
        "ax2.plot(df_plot.index, df_plot['Inyeccion_Baseline_MW'\n",
        "], label='Inyección Baseline', color='green', linestyle=':')\n",
        "ax2.plot(df_plot.index, df_plot['Inyeccion_APPR'\n",
        "], label='Inyección APPR Optimizada', color='purple', linewidth=2)\n",
        "ax2.axhline(CAPACIDAD_RESTRINGIDA, color='red', linestyle='--', label='Límite de Transmisión')\n",
        "ax2.set_title('Comparación de Despacho (Ejemplo de 1 Día Crítico)')\n",
        "ax2.set_ylabel('Potencia (MW)')\n",
        "ax2.legend(loc='upper right')\n",
        "ax2.grid(True, alpha=0.4)\n",
        "\n",
        "# Eje secundario para ver el estado de carga (SOC)\n",
        "ax3 = ax2.twinx()\n",
        "ax3.plot(df_plot.index, df_plot['SOC_t'], label='SOC Batería', color='orange', linestyle='--', alpha=0.5)\n",
        "ax3.set_ylabel('Energía (MWh)')\n",
        "ax3.legend(loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96ef02c7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- INICIALIZACIÓN DEL AGENTE PYTORCH ---\n",
        "state_size = env.state_space_size\n",
        "action_size = env.action_space_size\n",
        "\n",
        "agent = DQNAgent(state_size, action_size)\n",
        "print('Agente DQN (PyTorch) inicializado.')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}